\chapter{Background}\label{chap:background} \minitoc

\section{Stream processing as a superset of batch processing} \label{sec:stream-superset}

Batch processing is the processing of data grouped by batches. A batch is a collection of data points that have been grouped by certain criteria. Examples of such criteria could be \textit{"all the transactions from the past two months"}, \textit{"the last million transactions"} or even \textit{"all the transactions made by an user"}. Batch data sets are known as static data \cite{Martin-Batch-Defin}. An example application of such a technique would be processing all the keywords searched for in the google search engine. For instance, grouping the results by keyword and counting how many times a keyword was searched for trend analysis purposes. Batch processing is the oldest paradigm between batch and stream processing. Historically speaking, Apache Hadoop\footnote{https://hadoop.apache.org} was one of the first widely known technologies for large-scale batch processing, created in 2005. Nearly a decade later, Apache Spark\footnote{https://spark.apache.org} was released. Spark is capable of doing not only on-disk processing like Hadoop but also in-memory processing, achieving lower processing times. 

As more and more data became available in the data stream format, the need for stream processing increased. Stream processing refers to the processing of data streams. A data stream is essentially an unbounded data set. This is in contrast to batch processing which works on a finite data set. This distinction is important since some aggregations are impossible to compute with 100\% accuracy in an efficient manner on unbounded data sets. An example of such is counting distinct elements that flow through the data stream. An immediate solution would be to create a regular set of items or strings and have its length be the number of distinct elements processed. However, this approach is not very efficient and has a linear memory usage growth regarding the number of distinct elements. Therefore, as the number of distinct elements approaches large orders of magnitude we run out of memory.

In order to provide a streaming solution, Apache Spark created the Spark Streaming\footnote{https://spark.apache.org/docs/latest/streaming-programming-guide.html} module. Being a batch system in its core, Spark implements streaming as micro-batching. This approach handles streaming data by grouping incoming events into very small batches. Then, in order to process the entire defined window of data, it joins all micro-batches into a larger one and performs the desired computations. However, this creates an artificial barrier that does not really exist in the streaming context. More than conceptually different from a true stream processing system, processing a stream in a batch fashion, even if in small ones, presents some issues. First, since we are indeed working with batches, results will never be real-time updated per event. For example, if we use a batch size of 100 events, the system would only produce accurate results every 100 events. To handle system inactivity usually these micro-batch systems also have the notion of updating the aggregations with incomplete batches, if no event has entered the system after a pre-specified time period has elapsed. As such, micro-batching was created as a natural implementation of stream processing using the available batch processing systems of the time, but can not be considered true stream processing.

Distinguishing between batch and stream processing is important to understand how a monitoring system on top of the latter would work. The Venn diagram \ref{fig:stream-superset} \cite{batch-is-a-special-case-of-streaming} illustrates the relation of both processing techniques. A data stream is unbounded or infinite while a bounded data stream has a beginning and an end. Using these definitions, the concept of an unbounded data stream contains that of a bounded data set. That is, from an infinite stream, it is possible to fix a left and right limits in order to obtain a bounded and finite data set. This relation is visible in Figure \ref{fig:unbounded-bounded-superset} \cite{batch-is-a-special-case-of-streaming}.

\begin{figure}[!htb]
    \begin{center}
      \includegraphics[scale=0.5]{figures/streaming-subset-batch.png}
      \caption{Stream processing as a superset of batch processing}
      \label{fig:stream-superset}
    \end{center}
\end{figure}

\begin{figure}[!htb]
    \begin{center}
      \includegraphics[scale=0.5]{figures/unbounded-superset-bounded.png}
      \caption{Unbounded data streams as a superset of bounded data sets}
      \label{fig:unbounded-bounded-superset}
    \end{center}
\end{figure}


In Section 2.2 we will cover exactly how an unbounded data set (also known as a data stream) can be converted into a bounded data set with the use of windows.

\section{From unbounded streams to finite data sets} \label{sec:windows}

In Section 2.1 we have defined a data stream as an unbounded data set. But what exactly are the issues of working with an infinite stream of data? Why might we need to convert it into a bounded data set and how to do so? In this Section, we will analyze such issues and present streaming windows as the solution.

% boundless aggregations no need for windows
Computations over an unbounded data set are not impossible. For example, computing the \textit{average} of an event's field for all incoming events is computationally feasible. However, is this boundless average over the stream of events useful? While in some scenarios it may be, more often than not we would like to introduce some logical context into our computations. For instance, consider the monitoring of machines spread out across multiple racks in a large data center and its temperatures. Assume that temperatures measured across all racks are sent to a monitoring system in the form of a data stream. Is the average of all temperatures of all machines over an infinite period useful for the detection of overheating equipment? In this scenario, an average of the temperatures per rack and for the last 5 minutes allows for actionable context, in this case identifying a likely-to-fail rack. To obtain such insight requires computing the average over the last 5 minutes' worth of data, effectively defining a begin and end --- \textit{i.e.} all the data between the current timestamp in minutes \textit{t} and \textit{t-5}. By creating such a conceptual range, we realize that in practice we need a bounded data set. Partitioning an infinite data stream to obtain such a finite data set is made through the use of windows.

Gedik et. al claim that \textit{"recent history is often managed using windows"} and that \textit{"windowing makes it possible to implement streaming versions of the traditionally blocking relational operators, such as streaming a\cite{Wang-Windows-Stream-Processing}ggregations"} \cite{Wang-Windows-Stream-Processing}. More formally, Botan et. al define that \textit{"a window W over a stream S is a finite subset of S"} \cite{Botan-SECRET}.

% 2 dimensions of windows
Windows can be categorized by two dimensions: the window size \textit{W} and the sliding step size \textit{S}, both either tuple or time-based. Tuple-based windows' size is specified by the number of items the window can contain and time-based windows specify the time interval worth of incoming data a window can store. For instance, a tuple-based window of size $10^3$ tuples will store exactly $10^3$ items. In contrast, a time-based window storing the last 5 minutes of events can contain a varying number of tuples but will always keep the events arrived in the last 5 minutes. A window slides over incoming data and groups it, enabling us to compute aggregations over a defined and bounded set of events. The step size \textit{S} of a windowing technique is formally defined as \textit{"the distance between consecutive windows"} \cite{Botan-SECRET}. This means that two consecutive windows W\textsubscript{1} and W\textsubscript{2} are exactly \textit{S} units away. As mentioned, these units can be units of time or a number of tuples.

There are three types of windows: true sliding, stepping or tumbling windows.
% true sliding: unitary steps
A true sliding window is defined by any given window size \textit{W} and a unitary step (\textit{S}=1), which can be defined as time or tuple-based. At each step, the window advances one unit, which may be one event for tuple-based steps, or one time-unit whether that is in milliseconds, seconds, days, weeks or another magnitude of time, measured based on the system clock or event time. Such a true sliding movement can be observed in Figure \ref{fig:sliding-window} with a tuple-based window of size \textit{W} of three tuples and unitary step. 

\begin{figure}[!htb]
    \begin{center}
      \includegraphics[scale=0.3]{figures/sliding.png}
      \caption[True sliding window]{True sliding window, W=3, S=1}
      \label{fig:sliding-window}
    \end{center}
\end{figure}

% stepping windows (per x units over Y window size)
% stepping: step > 1 < window size
A stepping window behaves exactly like a true sliding one but the step size \textit{S} is not unitary. In other words, a stepping window is defined by any window size \textit{W} and steps \textit{S} greater than a unit. Once again, both the window and step sizes, \textit{W} and \textit{S} respectively, can be time or tuple-based. Figure \ref{fig:stepping-window} represents a stepping tuple-based window of size \textit{W} of three and a stepping size \textit{S} of two.

\begin{figure}[!htb]
    \begin{center}
      \includegraphics[scale=0.3]{figures/stepping.png}
      \caption[Stepping window]{Stepping window, W=3, S=2}
      \label{fig:stepping-window}
    \end{center}
\end{figure}

% tumbling: step = window size
A tumbling window, also defined by its window size \textit{W} and step size \textit{S}, is a window where W is equal to S. Hence, tumbling windows partition the stream into non-overlapping chunks with the same size. Since the intersection of different tumbling windows is always empty, each event is said to belong to exactly one tumbling window. An example of a tumbling window can be seen in Figure \ref{fig:tumbling-window} with a tuple-based window of size \textit{W} of three and an equally large step size \textit{S}.

\begin{figure}[!htb]
    \begin{center}
      \includegraphics[scale=0.3]{figures/tumbling.png}
      \caption[Tumbling window]{Tumbling window, W=S=3}
      \label{fig:tumbling-window}
    \end{center}
\end{figure}

The mathematical expression below summarizes our categorization of sliding windows.
\begin{equation} 
  Window \,\, of \,\, size \,\, W, \,\, step \,\, S =
    \begin{cases}
      true \,\, sliding & \text{if \textit{S}=1}\\
      stepping & \text{if \textit{S}>1}\\
      tumbling & \text{if \textit{S}=\textit{W}}
    \end{cases}
\end{equation}

For the scope of this Thesis, we will work under a tuple-based sliding window framework with a unitary step size. Despite focusing on tuple-based true sliding windows, our approach is just as valid for time-based windows and for other step values as well.

\section{Aggregations over data} \label{sec:aggregations}

An aggregation --- also known as a fold, reduce or accumulate --- is an operation that reduces a collection of values to a single one. Examples of such are the computation of data set sums, minima, averages or the number of distinct elements (cardinality). An aggregation may be exact or approximate. Exact aggregations compute a value from a sequence of elements, where no error or loss of resolution exists. On the other hand, approximate aggregations have an error margin to the associated aggregation value which is usually controllable by adjusting a set of parameters. 
Approximate aggregations are required when memory is limited, the volume of data to process is large and/or computing the exact aggregation can not be done in useful time. Thus, approximate aggregators trade precision of results for efficient time and space usage. In Section \ref{sec:pds} we explore approximate aggregators that make such a trade-off.

Aggregations are present in most programming languages and frameworks. Quoting from the Java programming language documentation on the \textit{Stream.reduce()} method\footnote{https://docs.oracle.com/en/java/javase/13/docs/api/java.base/java/util/stream/package-summary.html\#Reduction}: \textit{"A reduction operation (also called a fold) takes a sequence of input elements and combines them into a single summary result by repeated application of a combining operation"}. Similarly, Hadoop's MapReduce paradigm consists of first applying \textit{map} steps to the collection of data and then a \textit{reduce} one. \textit{Map} tasks will transform each item as desired and the \textit{reduce} task will merge the collection into a single value. Reduce methods make use of a \textit{combine} function and recursively apply it. The \textit{combine} function takes the current aggregation value and combines it with the next item in the set, returning the new aggregated state. Doing this recursively for each item in the collection results in a single value: the aggregate value.

\subsection{Sliding Window Aggregations}

While some aggregations can be computed on an infinite data set, often we want to compute these over a finite one. As we have seen in Section \ref{sec:windows}, to obtain a bounded data set from a data stream we apply windowing techniques. In doing so, we obtain a finite data set denoted as a sliding window and compute aggregations over it. Tangwongsan et. al define such aggregations over sliding windows as Sliding Window Aggregations (SWAGs) \cite{Tangwongsan-Sliding-Window-Aggregation-Algorithms}. Consider the data center temperature monitoring problem from Section \ref{sec:windows} as an example use-case for SWAGs. How would we compute the average temperature per rack in the data center in the last 5 minutes? To better understand, let us translate this question to a streaming SQL query, with a similar syntax to the API provided by Apache Flink\footnote{https://flink.apache.org}. Assuming all the temperatures can be queried from the \textit{Temperatures} table and that each rack corresponds a different \textit{rack\_id} we have the following query:

\begin{verbatim}
SELECT rack_id, AVG(temperature)
FROM Temperatures
GROUP BY TUMBLE(timestamp, Time.minutes(5), rack_id)
\end{verbatim}

In the query above, the \textit{TUMBLE(timestamp, Time.minutes(5), rack\_id)} statement groups incoming data using time-based tumbling windows of 5 minutes duration, using the \textit{timestamp} field to evict events not in this time interval. The resulting list of rows has \textit{rack\_id} as its key and as corresponding value a list of temperatures, from the past 5 minutes. Finally, for each rack and its list of temperatures, we compute an \textit{AVG} aggregation, resulting in the average temperature per rack in the past 5 minutes.


\subsection{Sliding Window Aggregation Algorithms} \label{sec:back-swag-algs}

A sliding window aggregation algorithm allows the computation of an exact or approximate aggregation over a sliding window. A SWAG algorithm may restrict the set of computable aggregations based on their properties. Different SWAG algorithms use different data structures for the window contents and the incremental aggregation state.

Two of the most basic SWAG algorithms are \textit{Recalculate-From-Scratch (RFS)} and \textit{Subtract-On-Evict (SOE)}.

Recalculate-From-Scratch (RFS) is the simplest and most general SWAG algorithm. As the name suggests, this algorithm recomputes the aggregation over the entire window for each update. For instance, the computation of a sum over a sliding window using RFS means that whenever the window changes all elements are summed. This approach works for every aggregation. However, given \textit{n} as the window size, this algorithm takes \textit{O(n)} space and time complexity which makes it unfit in most scenarios of large scale data processing. Figure \ref{fig:pseudo-rfs} shows pseudocode for RFS \cite{Tangwongsan-Sliding-Window-Aggregation-Algorithms}.



\begin{figure}[!htb]
    \begin{center}
      \includegraphics[scale=0.45]{figures/rfs.png}
      \caption{Recalculate-From-Scratch algorithm}
      \label{fig:pseudo-rfs}
    \end{center}
\end{figure}
 
Subtract-On-Evict (SOE) on the other hand, is a SWAG algorithm that incrementally computes the aggregation value. For each element insertion or eviction from the window, the aggregation value is updated, in contrast to RFS which would recompute the entire aggregation. SOE relies on expiring the evicted elements from the aggregation state. Hence, SOE only works for \textit{invertible} aggregations. To illustrate, consider the computation of a sum aggregation over a sliding window. The total sum of the window will be the aggregation state or value. When inserting an element, the updated total window sum equals the current total sum plus the new element. The sum aggregation is considered \textit{invertible} because, when evicting an element, the new aggregation state is the total sum computed thus far minus the evicted element. With window size of \textit{n}, SOE has \textit{O(n)} space complexity and \textit{O(1)} time complexity. However, aggregations are not always invertible thus making this algorithm very restrictive and not always applicable. Figure \ref{fig:pseudo-soe} shows pseudocode for SOE \cite{Tangwongsan-Sliding-Window-Aggregation-Algorithms}.

\begin{figure}[!htb]
    \begin{center}
      \includegraphics[scale=0.45]{figures/soe.png}
      \caption{Subtract-On-Evict algorithm}
      \label{fig:pseudo-soe}
    \end{center}
\end{figure}

Subtract-On-Evict is an improvement over the Recalculate-From-Scratch algorithm that  based on the idea of aggregation invertibility. The question then becomes if there are other aggregation properties besides invertibility that must be taken into account when analyzing general stream processing algorithms.


\subsection{Aggregation properties}
\label{sec:agg-properties}

Tangwongsan et al. construct Table \ref{tbl:aggregations-properties} to summarize the aggregation families and satisfied properties \cite{Tangwongsan-Sliding-Window-Aggregation-Algorithms}.

%TODO: change the sketch-like from the aggregator to the aggregation: bloom filter to membership query, CMS to frequency estimation and HLL to cardinality estimation
\begin{table}[!htb]
    \begin{center}
      \includegraphics[scale=0.4]{figures/aggregation-operations-properties.png}
      \caption[Aggregation properties]{Aggregation properties. Checkmarks (\checkmark), crosses ($\times$), and question marks (?) indicate a property is true for all, false for all, or false for some of a given group of like operations, respectively}
      \label{tbl:aggregations-properties}
    \end{center}
\end{table}

To understand these properties, we must be familiar with the \textit{combine} and \textit{lower} functions. The already mentioned \textit{combine} function merges the incoming event with the incrementally computed aggregation state. The \textit{lower} function is responsible for returning a result for a given query according to the current aggregation state. For instance, consider once again the use case of computing a sum aggregation over a window of data. The \textit{combine} function would be the arithmetic operator \textit{+} and would merge the current aggregation state --- \textit{i.e.} the current sum value --- with the new incoming event. The \textit{lower} function would return the current aggregation state --- \textit{i.e.} return the total sum computed.

Based on the \textit{combine} and \textit{lower} functions, the following are formal definitions of these properties. An aggregation is said to have:

\begin{enumerate}
    \item  an \textbf{\textit{invertible combine}} if a function $\ominus$ exists such that $(x \oplus y) \ominus y = x$, for all $x$ and $y$
    
    \item  an \textbf{\textit{associative combine}} if $(x \oplus y) \oplus z = x \oplus (y \oplus z)$, for all $x$ and $y$
    
    \item  a \textbf{\textit{commutative combine}} if $x \oplus y = y \oplus x$, for all $x$ and $y$
    
    \item  a \textbf{\textit{size-preserving combine}} if the aggregation state resulting from $\oplus$ always occupies the same space in memory
   
    \item  a \textbf{\textit{unary lower}} if the query does not need any parameters --- \textit{e.g.} a membership test with a Bloom filter would require the element to test membership for as a parameter so it is not unary; returning the total sum is unary since it does not require any parameters
\end{enumerate}


The aggregation properties analyzed in this Section will be used to evaluate SWAG algorithms. Different algorithms might only apply to certain families of aggregations. This way, the multiple SWAG algorithms presented throughout this Thesis will be analyzed not only on their time and space complexity but also on the restrictions they impose regarding the aggregation' properties required. For example, under this framework, to implement Subtract-On-Evict (SOE), both the \textit{combine} $\oplus$ and its inverse $\ominus$ functions must be implemented so that SOE applies $\ominus$ to expire the evicted element from the aggregation state. Since SOE only restriction is for the aggregation to be invertible it works for the \textit{sum-like}, \textit{collect-like} and \textit{median-like} families in Table \ref{tbl:aggregations-properties}.

%\section{Aggregations on data types}

%Some aggregations are only possible on certain data types. For instance, computing an average on a set of strings is not possible. In this Section we present the data types we will work with and the set of possible aggregations to compute on them.

%We consider four data types: \textit{numericals}, \textit{strings}, \textit{dates} and \textit{categoricals}. Numerical variables can be measured. When compared to classical programming data types, for example those of the C programming language, numericals comprise variable types \textit{short}, \textit{long}, \textit{int}, \textit{float}, \textit{double} and valid combinations of the previous --- \textit{e.g.} \textit{long int}. On the other hand, Strings are sequences of characters. The \textit{string} type is the most generic data type in that any other type can be converted to a string. Every variable, for example \textit{x}=3, can be turned into a string \textit{s}, for example \textit{s}="3". Similarly, a date is a sequence of characters and thus a string. However, despite being a string, it has semantics unlike a regular one. Knowing the date format, one can extract the day, month and year. For that reason we make the distinction between the \textit{string} and \textit{date} data types. Lastly, a \textit{categorical} is a variable that takes one of a relatively small and known set of possible values. A variable is assigned to a particular category based on its properties. The categorical variable is also a sequence of characters and therefore a string variable. The main difference between the categorical data type and the string is that we know the set of possible labels the categorical variable might take and this affects the type of computable aggregations.

%On a set of numerical variables, we can compute sums, minima, maxima, averages, standard deviations and counts (of zeros, infinities and others). When operating on a set of strings, we can aggregate the collection into a concatenation of strings, a count, a membership query, cardinality.


\section{Exponential Moving Averages}
Both SOE and RFS require saving the entire window contents. IS THERE A WAY WE CAN EMULATE AN WINDOW....

