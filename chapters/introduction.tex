\chapter{Introduction} \label{chap:intro} \minitoc

\section{Context} \label{sec:context}
In the past decade, applications have become increasingly data-driven, placing data at the center of application design. Different use cases process data in different ways for different purposes. For instance, e-commerce platforms need to process a large number of transactions, for example, those made daily by all Alibaba's clients while ensuring that sales run smoothly and that products are delivered to customers' homes. Streaming services, like YouTube and Netflix, try to guarantee that media content reaches up to millions of users simultaneously. Furthermore, this media content may have been previously recorded and stored, or may be generated on the fly. Social networks are also responsible for generating large volumes of data. A social network like Twitter, for example, generates more than 500 million \textit{tweets} (posts in Twitter) per day \cite{twitter-throughput}. \textit{Tweets} may contain text, media content or both. Cybersecurity applications are yet another example of time-sensitive, data-driven applications. These need to monitor user accesses and their actions in the network, where timely detection of intruders is critical to prevent them from tampering with the protected system.

All of these applications generate large volumes of data over time which poses data storage and processing challenges. This data is characterized in terms of its large volume, high velocity and high variety \cite{Mavragani-GoogleTrends-SLR}, creating large scale data management problems. Additionally, the large amounts of information produced by such systems and use-cases lead to the creation of multiple real-time unbounded data sets, also known as data streams. The information that flows through such a stream can be analyzed on-the-fly or stored for later processing. The former case is best known as stream processing, while the latter as batch processing. E-commerce applications have to process endless streams of credit card transactions. Live streaming services need to distribute the media content created by the \textit{streamer} (content creator) and deliver it across multiple users. Social networks process and analyze hundreds of posts per second to find out trending keywords across their network and suggest them to users. Computer network monitor systems want to detect intruders and revoke them system access before they have time to corrupt the system or steal private information. All of the previously given use cases rely on real-time processing of data for near real-time actions. Hence, between batch and stream processing, the latter fits these use cases better.

As mentioned, the processing of data can be divided into two main categories: batch and stream processing. However, due to the increasing volume of data and the need for timely processing to allow companies to react to changing conditions in real-time, stream processing demand is increasing. Data stream processing refers to the handling of huge volumes of high variety of data generated at high-velocity from multiple sources in real-time. Unlike batch processing on static data sets, where assumptions on the underlying data distribution can be made, streaming data is known for being non-stationary \cite{Gama-Knowledge-Discovery}. Furthermore, in the streaming paradigm, the value of the produced information lies in its recency \cite{Kolajo-Big-data-stream-SLR}. \textit{Recency} is measured under different scales for different use cases. For instance, considering the monitoring of geological data to forecast possible natural disasters such as earthquakes versus the monitoring of a computer network for intruder detection. In the former case, after the forecast of a future earthquake, governmental authorities need a couple of days to launch an evacuation plan and keep the population safe. However, in the intruder detection scenario, the decision of removing access to a user must be done as soon as possible, preferably in a few seconds. Hence, while in the first scenario information retrieved within a day would still be recent, in the second scenario, information is considered recent and relevant if delivered within seconds (or even milliseconds).

Both in batch and stream processing, we usually want to apply operations on data, for example, to compute the maximum value, an average or a count of distinct elements. Operations applied to data sets that produce a single result are commonly known as aggregate operations. In stream processing, we apply these aggregations to data streams to obtain valuable information. However, since a data stream is, effectively speaking, an unbounded data set, if our aggregation requires a finite domain, there is the need to apply windowing techniques. A thorough analysis to stream windowing techniques is done in the Background Section of this document but for now it suffices to say that windowing transforms an unbounded data stream into a finite data set by defining a time or tuple-based window size --- \textit{i.e.} events from the last three days or the last $10^6$ events, respectively.

Consider the case is the detection of credit card fraud on the data stream of all the transactions made on Amazon by every US citizen on a single day. In this case, Amazon's transaction stream will need to be monitored by another system, a fraud detection one. Such a system would have to process all the incoming transactions and decide in a fraction of a second if it is a fraudulent one or not. It is clear that manually monitoring such an intensive, latency-sensitive data stream is hard and error-prone. Thus justifying the need for autonomous real-time data stream monitoring systems.

Many market solutions monitor streams of credit card transactions with the intent of fraud detection. In the context of this Thesis, we will work side by side with Feedzai. Feedzai offers fraud prevention solutions for the previously presented use case of monitoring an infinite stream of credit card transactions and detecting fraud. We will work side by side with them, taking advantage of the multiple real-life data sets from the financial fraud space they can provide. 

Feedzai's solution for fraud detection consists of a workflow of different components. Incoming events are fed into this workflow and processed accordingly. Workflow components can be sets of user-defined rules and/or Machine Learning (ML) models that output scores reflecting their belief on whether a transaction is fraudulent or not. ML models are configured and trained before deployment making assumptions on the statistical distributions of future incoming data. Thus their model performance is linked to the maintenance of these assumptions. However, fraud patterns are not static over time, but seasonal --- \textit{e.g.} Black Fridays, holiday shopping and product launches --- and evolutionary --- \textit{e.g.} new fraud attack methods by fraudsters. Due to these shifts in data patterns and distributions, the ML models' performance can deteriorate over periods of time. In the Machine Learning field, the performance decay caused by data pattern shifts is also known as concept drift. %UNCOMMENT WHEN BACKGROUND SECTION IS DONE: Such a concept will be further explained in Section \ref{sec:concept-drift}.

\section{Objectives} \label{sec:objectives}
Feedzai's data pattern shift challenge is not unique to them nor their field. Many analytical systems are static once deployed and are configured \textit{a priori} under the assumption that future data flowing through the system will roughly follow the same distribution as previously seen data.
The problem is that data pattern shifts will lead to poor performance by the previously configured system. Monitoring the data patterns themselves and alerting for changes in the underlying distribution would allow users to reconfigure the system before its performance declines.

The objective of this Thesis is to build a lightweight real-time system that detects these shifts in data patterns in systems that handle high volumes of high velocity, highly skewed and seasonal data.

\section{Motivation} \label{sec:motivation}
For any organization, the quality of their provided services is paramount. Therefore, it is common for companies to have systems monitoring the former, testing their usage rate, availability and performance, just to name a few. The motivation behind this Thesis is to produce a system capable of real-time monitoring a stream of data so that changes in the underlying stream are caught early, preventing a fall in performance. 

We want to build a lightweight enough solution that can be integrated into existing workflows. Ensuring constant time and space complexity means that the system will remain usable for the ever-growing volumes of data. Since such a monitoring system is not mission-critical --- \textit{i.e.} the provided services don't rely on it --- the operational cost associated with it must be kept to a minimum. Having a real-time system monitoring other real-time systems consuming as many resources as the latter would be far too costly.


\section{Hypothesis} \label{sec:hypothesis}
The question this Thesis aims to answer is whether sliding window aggregations and probabilistic data structures can be used to detect data pattern shifts in data streaming scenarios in the presence of high volumes of high velocity, highly skewed and seasonal data with a low memory footprint.

\section{Hypothesis Validation} \label{sec:validation}
The hypothesis will be proven true or false depending on whether the following criteria are met. The resulting data pattern shift reporting system must have the following properties:
\begin{itemize}
    \item high precision ratio --- \textit{i.e.} at least 80\%
    \item high recall ratio --- \textit{i.e.} at least 80\%
    \item low latency --- \textit{i.e.} constant time complexity, \textit{O(1)}
    \item memory efficiency --- \textit{i.e.} constant memory complexity, \textit{O(1)}
\end{itemize}

Experiments will be made using multiple real data sets from the financial fraud space and compared to existing batch analysis results.

\section{Document structure}
This section merely sums up how each of the following Chapters contributes to this Thesis.

In Chapter \ref{chap:background} of this document we brief the reader with the necessary background and relevant concepts needed to understand the following Chapters.

In Chapter \ref{chap:sota} we examine the state of the art in the area, analyzing the most recent work done and classifying it into subgroups.

%In Chapter \ref{chap:approach} we define the scope of this project, the research questions we intend to answer, our experimental methodology and our future planning.

In Chapter \ref{chap:conclusion} we summarize our contributions.